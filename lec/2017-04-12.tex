\documentclass[12pt, leqno]{article} %% use to set typesize 
\input{common}

\begin{document}
\hdr{2017-04-12}

\section{Life beyond fixed point iterations}

So far, the methods we have discussed for solving nonlinear systems
all involve some flavor of fixed point iteration
\[
  x_{k+1} = G(x_k).
\]
Our chief example of such an iteration is Newton's method, where
\[
  G(x) = x - f'(x)^{-1} f(x),
\]
but we have also considered various other iterations where the
Jacobian is replaced by some more convenient approximation.  However,
all the methods we have discussed in this setting compute the next
point based {\em only} on the behavior at the current point, and not
any previous points.

In earlier parts of the class, of course, we {\em did} consider
iterations where the next approximation depends on more than just
the current approximation.  Two such iterations spring particularly
to mind:
\begin{itemize}
\item The {\em secant method} is the 1D iteration in which we approximate
  the derivative at the current point by a finite difference through
  the last two points; that is,
  \[
    x_{k+1} = x_k - \frac{x_k-x_{k-1}}{f(x_k)-f(x_{k-1})} f(x_k).
  \]
  Hence, the secant method depends not only on the behavior at
  the current point $x_k$, but also on the previous point $x_{k+1}$.
  The secant method, along with bisection, is the building block for
  Brent's method, the basis of the \matlab\ {\tt fzero} function,
  which is at once robust, rapidly convergent, and derivative free.

\item In iterative methods for solving linear systems, we started
  with fixed point iterations based on matrix splittings (such as
  Jacobi and Gauss-Seidel), but then recommended them primarily as
  an approach to preconditioning the more powerful Krylov subspace
  methods.  One way to think about (preconditioned) Krylov subspaces
  is that they are spanned by the iterates of one of these stationary
  methods.  Taking a general element of this space --- a linear
  combination of the iterates of the stationary method --- improves
  the convergence of stationary methods.

\end{itemize}

In this lecture, we describe nonlinear analogues to both of these
approaches.  The generalization of the secant condition to more
than one direction gives {\em Broyden's method}, the most popular of
the {\em quasi-Newton} methods that couple a Newton-like update of
the approximate solution with an update for an approximate Jacobian.
The generalization of the Krylov subspace approach leads us to
{\em extrapolation methods}, and we will describe a particular example
known as {\em reduced rank extrapolation}.  We conclude the lecture
with a brief discussion of {\em Anderson acceleration}, a method that
can be seen as a generalization either of the secant approach or
of Krylov subspace methods.

\section{Broyden}

Quasi-Newton methods take the form
\[
  x_{k+1} = x_k - J_k^{-1} f(x_k)
\]
together with an updating formula for computing successive approximate
Jacobians $J_k$.  By far the most popular such updating formula is
{\em Broyden's (good) update}\footnote{Broyden proposed two updates in
  his original formula; the one that has come to be known as the
  ``good'' update typically behaves much better than the ``bad''
  update.}:
\[
  J_{k+1} = J_k + \frac{f(x_{k+1}) s_k^T}{\|s_k\|^2}, \quad s_k \equiv x_{k+1}-x_k.
\]
Broyden's update satisfies the {\em secant condition}
\[
  J^{k+1} (x_{k+1}-x_k) = f(x_{k+1})-f(x_k),
\]
which is the natural generalization of the 1D secant condition.
In one dimension, the secant condition is enough to
uniquely determine the Jacobian approximation, but this is not the
case in more than one dimension.  Hence, Broyden's update gives the
matrix $J^{k+1}$ that minimizes $\|J^{k+1}-J^k\|_F^2$ subject to the
secant condition.

Broyden's method has three particularly appealing features:
\begin{itemize}
\item
  For sufficiently good starting points $x^0$ (and sufficiently
  innocuous initial Jacobian approximations), Broyden's method is
  {\em $q$-superlinearly} convergent, i.e. there is some constant
  $K$ and some $\alpha > 1$ such that
  \[
    \|e^{k+1}\| \leq K \|e^k\|^\alpha.
  \]
\item
  The iteration requires only function values.  There is no need
  for analytical derivatives.
\item
  Because each step is a rank one update, we can use linear algebra
  tricks to avoid the cost of a full factorization at each step that
  would normally be required for a Newton-type iteration.
\end{itemize}

The argument that Broyden converges superlinearly is somewhat complex,
and we will not cover it in this course; for details of the argument,
I suggest
Tim Kelley's {\em Iterative Methods for Linear and Nonlinear Equations}.
The fact that the method requires only function values is obvious from
the form of the updates.  But the linear algebra tricks bear some
further investigation, in part because different tricks are used
depending on the type of problem involved.

\subsection{Dense Broyden}

For moderate-sized problems, implementations of Broyden's method
may maintain a dense factorization of the approximate Jacobian
which is updated at each step.  This updating can be done economically
by exploiting the fact that the update at each step is rank one.
For example, the QR factorization of $J_k$ can be updated to a QR
factorization of $J_{k+1}$ in $O(n^2)$ time by using a sequence of
Givens rotations.  More generally, we can compute the QR factorization
of the rank one update $A' = A + uv^T$ from the QR factorization
$A = \bar{Q} \bar{R}$ in three steps:
\begin{enumerate}
\item
  Write $A' = \bar{Q} (\bar{R} + \bar{u} v^T)$
  where $\bar{u} = \bar{Q}^T u$.
\item
  Apply $n-1$ Givens rotations to $\bar{u}$ to introduce zeros in
  position $n, n-1, \ldots, 2$.  Apply those same rotations to the
  corresponding rows of $R$, transforming it into an upper Hessenberg
  matrix $H$; and apply the transposed rotations to the corresponding
  columns of $\bar{Q}$ to get $A' = \tilde{Q} (H + \gamma e_1 v^T)$.
\item
  Do Hessenberg QR on $H + \gamma e_1 v^T$ to finish the
  decomposition, i.e.~apply run Givens rotations to zero out
  each subdiagonal entry in turn
\end{enumerate}
In total, this computation involves computing $\bar{u}$ (in $O(n^2)$
time) and applying $2n-2$ Givens rotations across the two factors
(also a total of $O(n^2)$ time).  So updating the factorization of
$J_k$ to get a factorization of $J_{k+1}$ takes $O(n^2)$; and once we
have a factorization of $J_{k+1}$, linear solves can be computed in
$O(n^2)$ time as well.  So the total cost per Broyden step is
$O(n^2)$.

\subsection{Large-scale Broyden}

For large-scale problems, the $O(n^2)$ time cost of a dense Broyden
update is prohibitive --- or perhaps the $O(n^2)$ storage cost is the
pain.  Either way, we would like an alternative with lower complexity.
As long as we have a fast solver for the initial Jacobian $J_0$, we
can compute the first several Broyden steps using the
Sherman-Morrison-Woodbury formula or using a bordered linear system:
\[
  \begin{bmatrix}
    J_0 & -F_k \\
    S_k^T & D_k
  \end{bmatrix}
  \begin{bmatrix} s_{k} \\ \mu \end{bmatrix} =
  \begin{bmatrix} -f(x_k) \\ 0 \end{bmatrix},
\]
where
\begin{align*}
  F_k &= \begin{bmatrix} f(x_1) & \ldots & f(x_k) \end{bmatrix}, \\
  S_k &= \begin{bmatrix} s_0 & \ldots & s_{k-1} \end{bmatrix}, \\
  D_k &= \operatorname{diag}\left( \|s_0\|^2, \ldots, \|s_{k-1}\|^2 \right).
\end{align*}
Defining vectors $g_k = -J_0^{-1} f(x_k)$, we can rewrite the system
as
\[
  \begin{bmatrix}
    I     & G_k \\
    S_k^T & D_k
  \end{bmatrix}
  \begin{bmatrix} s_{k} \\ \mu \end{bmatrix} =
  \begin{bmatrix} g_k \\ 0 \end{bmatrix}.
\]
Defining $w = -\mu$ and performing block Gaussian elimination then yields
\begin{align*}
  (D_k - S_k^T G_k) w &= S_k^T g_k \\
  s_k &= g_k + G_k w.
\end{align*}
Hence, to go from step $k$ to step $k+1$ in this framework requires
one new solve with $J_0$, $O(k^2)$ time to solve the Schur complement
system using an existing factorization, and $O(nk)$ time to form the
new step and extend the Schur complement system for the next solve.
As long as $k$ is modest, this is likely an acceptable cost.  For
larger $k$, though, it may become annoying.  Possible solutions
include {\em limited memory Broyden}, which only takes into account
the last $m$ steps of the iteration when computing the modified
Jacobian; or {\em restarted Broyden}, which restarts from an
approximate Jacobian of $J_0$ after every $m$ Broyden steps.

\section{Reduced rank extrapolation}

{\em Extrapolation methods} are sequence transformations that convert
a slowly-converging sequence into one that is more rapidly convergent.
For reasons that are perhaps as much cultural as technical, these
methods are often not covered in most introductory numerical analysis
texts and courses, but they can be extremely powerful when applied
correctly.

We will be concerned with {\em vector extrapolation methods}, which
apply to a vector sequence; popular examples include reduced rank
extrapolation, minimal polynomial extrapolation, and vector Pad\'e
methods.  We will focus on the example of reduced rank extrapolation (RRE),
but all of these methods have a similar flavor.  We suppose
$x_1, x_2, \ldots \in \bbR^n$ converges to some limit $x_*$, albeit slowly.
We also suppose an {\em error model} that describes $e_k = x_k-x_*$;
in the case of RRE, the error model is
\[
  e_k \approx \sum_{j=1}^m u_j \alpha_j^k.
\]
If this error model were exact (and if we knew the $\alpha_j$),
we could define a degree $m+1$ polynomial
\[
  p(z) = \sum_{i=0}^m \gamma_i z^i
\]
such that $p(z) = 1$ and $p(\alpha_j) = 0$ for each $j$.  Then
\begin{align*}
  \sum_{i=0}^m \gamma_i x_{k+i}
  &= \sum_{i=0}^m \gamma_i \left(x_* + \sum_{j=1}^m u_j \alpha_j^{k+i}\right) \\
  &= p(1) x_* + \sum_{j=1}^m u_j \alpha_j^k p(\alpha_j) \\
  &= x_*.
\end{align*}
Of course, in practice, the error model is not exact, and we do not
know the $\alpha_j$!  Nonetheless, we can come up with an appropriate
choice of polynomials by asking for coefficients $\gamma$ such that
\[
  r_k =  \sum_{i=0}^m \gamma_i x_{k+i+1} - \sum_{i=0}^m \gamma_i x_{k+i}
\]
is as small as possible in a least squares sense;
writing $u_{k+i} = x_{k+i+1}-x_{k+i}$ and
$U_k = \begin{bmatrix} u_k & \ldots & u_{k+m} \end{bmatrix}$, we then have
\[
  \mbox{minimize } \|U_k \gamma\|_2^2 \quad \mbox{s.t. } \sum_i \gamma_i = 1.
\]
If $U_k = Q_k R_k$ is an economy QR decomposition, the solution to
this minimization problem is
\[
  \gamma = (R_k^{-1} y)/\|y\|^2, \quad y = R_k^{-T} e, \quad e
  = \begin{bmatrix} 1 \\ \vdots \\1 \end{bmatrix}.
\]
Moreover, we can compute the QR decomposition of $U_{k+1}$ from the QR
decomposition of $U_k$ relatively quickly (in $O(nm)$ time).  Hence,
each time we want to compute one more vector in the extrapolated
sequence, the cost is that of forming one more vector in the original
sequence followed by $O(nm)$ additional work.

When applied to a the iterates of a stationary iterative method for
solving $Ax = b$, reduced rank extrapolation is formally the same as
the GMRES iteration.  In numerical practice, though, the
orthogonalization that takes place when forming the Krylov subspace
bases in GMRES provides much better numerical stability than we would
get from RRE.  Whether we apply the method to linear or nonlinear
problems, the matrices $U_k$ in RRE are often rather ill conditioned,
and the coefficient vectors $\gamma$ have large positive and negative
entries, so that forming $\sum_i \gamma_i x_{k+i}$ may lead to
significant cancellation.  For this reason, one may want to choose
modest values of $m$ in practice.

\section{Anderson acceleration}

{\em Anderson acceleration} is an old method, common in computational
chemistry, that has recently become popular for more varied problems
thanks to the work of Tim Kelley, Homer Walker, and various others.
In many ways, it is similar to reduced rank extrapolation applied to
a sequence of iterates from a fixed point iteration.  If we wish to
find $x_*$ such that
\[
  f(x_*) = g(x_*) - x_* = 0,
\]
then reduced rank extrapolation --- without any linear algebraic
trickery --- at step $k$ looks like
\begin{align*}
  m_k & = \min(m,k) \\
  F_k &= \begin{bmatrix} f(x_{k-m_k}) & \ldots & f(x_k) \end{bmatrix} \\
  \gamma^{(k)} &= \operatorname{argmin} \|F_k \gamma\|_2 \mbox{ s.t. }
  \sum_{i=0}^{m_k} \gamma_i^{(k)} = 1 \\
  s_{k+1} &= \sum_{i=0}^{m_k} \gamma_i^{(k)} g(x_{k-m_k+i})
\end{align*}
In this application of reduced rank extrapolation, the output sequence
$s_k$ and the input sequence $x_k$ (defined by the iteration
$x_{k+1} = g(x_k)$) are distinct entities.  By contrast, in Anderson
acceleration we have just {\em one} sequence:
\begin{align*}
  m_k & = \min(m,k) \\
  F_k &= \begin{bmatrix} f(x_{k-m_k}) & \ldots & f(x_k) \end{bmatrix} \\
  \gamma^{(k)} &= \operatorname{argmin} \|F_k \gamma\|_2 \mbox{ s.t. }
  \sum_{i=0}^{m_k} \gamma_i^{(k)} = 1 \\
  x_{k+1} &= \sum_{i=0}^{m_k} \gamma_i^{(k)} g(x_{k-m_k+i})
\end{align*}
The only difference in the pseudocode is that the last step generates
a new $x_{k+1}$ (which feeds into the next iteration), rather than
generating a separate output sequence $s_{k+1}$ and computing the next
input iterate by a fixed point step.
Thus, the difference between Anderson acceleration and reduced rank
extrapolation is morally the same as the difference between
Gauss-Seidel and Jacobi iteration: in the former case, we try to work
with the most recent guesses available.

Unsurprisingly, Anderson acceleration (like RRE) is equivalent to
GMRES when applied to linear fixed point iterations.  Anderson
acceleration can also be seen as a multi-secant generalization of
Broyden's iteration.  For a good overview of the literature and of
different ways of thinking about the method (as well as a variety of
interesting applications), I recommend ``Anderson Acceleration for
Fixed-Point Iterations'' by Walker and Ni (SIAM J.~Numer.~Anal.,
Vol.~49, No.~4, pp.~1715--1735).

\end{document}
